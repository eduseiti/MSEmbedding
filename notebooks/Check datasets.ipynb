{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import math\n",
    "\n",
    "import os\n",
    "import gc\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DATASET=\"/media/eduseiti/bigdata02/unicamp/doutorado/bootstrap.pytorch/data/mixedSpectraCrux/sequences/train_mixedSpectraCrux_v6.2.pkl\"\n",
    "\n",
    "ONE_EXPERIMENT=\"/media/eduseiti/bigdata02/unicamp/doutorado/bootstrap.pytorch/data/mixedSpectraCrux/sequences/Fetal_Testis_Gel_Velos_27_crux_q0.01_pvalue-63.91983.pkl\"\n",
    "\n",
    "VALIDATION_DATASET=\"/media/eduseiti/bigdata02/unicamp/doutorado/bootstrap.pytorch/data/mixedSpectraCrux/sequences/test_mixedSpectraCrux_v6.2.pkl\"\n",
    "\n",
    "TEST_DATASET=\"/media/eduseiti/bigdata02/unicamp/doutorado/bootstrap.pytorch/data/linfeng_all_q0.01_cell_state/sequences/sample_experiment_v6.2.pkl\"\n",
    "\n",
    "OUTPUT_FOLDER=\"/media/eduseiti/data_storage_1TB/unicamp/clustering_linfeng_sample_pvalues/datasets/analysis\"\n",
    "\n",
    "TRAIN_DATASET_MZ_INFO=\"train_6.2_mz_info.pkl\"\n",
    "TRAIN_DATASET_INTENSITY_INFO=\"train_6.2_intensity_info.pkl\"\n",
    "TRAIN_DATASET_PEPMASS_INFO=\"train_6.2_pepmass_info.pkl\"\n",
    "\n",
    "VALIDATION_DATASET_MZ_INFO=\"validation_6.2_mz_info.pkl\"\n",
    "VALIDATION_DATASET_INTENSITY_INFO=\"validation_6.2_intensity_info.pkl\"\n",
    "VALIDATION_DATASET_PEPMASS_INFO=\"validation_6.2_pepmass_info.pkl\"\n",
    "\n",
    "TEST_LINFENG_DATASET_MZ_INFO=\"test_linfeng_6.2_mz_info.pkl\"\n",
    "TEST_LINFENG_DATASET_INTENSITY_INFO=\"test_linfeng_6.2_intensity_info.pkl\"\n",
    "TEST_LINFENG_DATASET_PEPMASS_INFO=\"test_linfeng_6.2_pepmass_info.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_OF_MZ_BINS=2001\n",
    "NUM_OF_INTENSITIES_BINS=10000\n",
    "NUM_OF_INTENSITIES_BINS_FOR_CHART = 1000\n",
    "MAX_COUNT=10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PERCENTILES_TO_CALCULATE = list(np.arange(0.0, 105.0, 5))\n",
    "\n",
    "PERCENTILES_TO_CALCULATE += [99.0, 99.9, 99.99, 99.999]\n",
    "PERCENTILES_TO_CALCULATE.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_data(data_np, percentiles=PERCENTILES_TO_CALCULATE, histogram_bins=1000, output_filename=None, chart=True):\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    results['percentiles'] = list(zip(percentiles, np.percentile(data_np, percentiles)))\n",
    "    results['mean'] = np.mean(data_np)\n",
    "    results['std'] = np.std(data_np)\n",
    "    results['max'] = np.amax(data_np)\n",
    "    results['min'] = np.amin(data_np)\n",
    "    results['histogram'], results['bin_edges'] = np.histogram(data_np, histogram_bins)\n",
    "    \n",
    "    for i in range(len(results['percentiles'])):\n",
    "        print(\"Percentil={}, value={}\".format(results['percentiles'][i][0], results['percentiles'][i][1]))\n",
    "        \n",
    "    print(\"Data mean: {}\".format(results['mean']))\n",
    "    print(\"Data std: {}\".format(results['std']))\n",
    "    print(\"Data max: {}\".format(results['max']))\n",
    "    print(\"Data min: {}\".format(results['min']))\n",
    "    print(\"Total data points: {}\".format(data_np.shape[0]))\n",
    "    \n",
    "    if chart:\n",
    "        fig = go.Figure()\n",
    "\n",
    "        fig.add_trace(go.Bar(y=results['histogram'],\n",
    "                             x=results['bin_edges'][1:],\n",
    "                             marker_color='red'))\n",
    "\n",
    "        fig.show()\n",
    "    \n",
    "    if output_filename:\n",
    "        with open(output_filename, \"wb\") as outputFile:\n",
    "            pickle.dump(results, outputFile, pickle.HIGHEST_PROTOCOL)  \n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_and_describe_dataset(which_dataset, num_of_mz_bins=NUM_OF_MZ_BINS, num_of_intensities_bins=NUM_OF_INTENSITIES_BINS):\n",
    "    all_mz = []\n",
    "    all_intensities = []\n",
    "    \n",
    "    peaks_count = []\n",
    "    \n",
    "    for key, same_sequence in which_dataset['spectra'].items():\n",
    "        for spectrum in same_sequence:\n",
    "            all_mz.append(spectrum['nzero_peaks'][:, 0] * which_dataset['normalizationParameters']['mz_std'] + which_dataset['normalizationParameters']['mz_mean'])\n",
    "            all_intensities.append(spectrum['nzero_peaks'][:, 1] * which_dataset['normalizationParameters']['intensity_std'] + which_dataset['normalizationParameters']['intensity_mean'])\n",
    "            peaks_count.append(spectrum['nzero_peaks'].shape[0])\n",
    "            \n",
    "    print (\"m/z length={}\".format(len(all_mz)))\n",
    "    print (\"intensities length={}\".format(len(all_intensities)))\n",
    "\n",
    "    all_mz_concatenated = torch.cat(all_mz)\n",
    "    all_mz_np = all_mz_concatenated.numpy()\n",
    "\n",
    "    all_intensities_concatenated = torch.cat(all_intensities)\n",
    "    all_intensities_np = all_intensities_concatenated.numpy()\n",
    "    \n",
    "    all_peaks_count_np = np.array(peaks_count)\n",
    "    \n",
    "    print(\"\\n\\nm/z information\\n\")\n",
    "    mz_results = describe_data(all_mz_np, histogram_bins = num_of_mz_bins, chart = False)\n",
    "    \n",
    "    print(\"\\n\\nIntensities information\\n\")\n",
    "    intensities_results = describe_data(all_intensities_np, histogram_bins = num_of_intensities_bins, chart = False)\n",
    "\n",
    "    print(\"\\n\\nPeaks count information\\n\")\n",
    "    peaks_count_results = describe_data(all_peaks_count_np, chart = False)   \n",
    "    \n",
    "    return all_mz_np, all_intensities_np, mz_results, intensities_results, peaks_count_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Group intensities arrays into m/z bins\n",
    "#\n",
    "# Receives two main parameters \"which_mz\" and \"which_intensities\" arrays, which for each index contains the\n",
    "# m/z and the corresponding intensity reading for an entire dataset.\n",
    "#\n",
    "# Outputs a list of \"num_of_mz_bins\" length, which corresponds to m/z nominal masses; each list position contains an\n",
    "# array with all the intensities reading at that given m/z.\n",
    "#\n",
    "\n",
    "def allocate_intensities_to_mz_bins(which_mz, which_intensities, num_of_mz_bins=NUM_OF_MZ_BINS, chart=True):\n",
    "    mz_bins = [[] for i in range(num_of_mz_bins)]\n",
    "    \n",
    "    for i in range(which_mz.shape[0]):\n",
    "        mz_bins[int(round(which_mz[i]))].append(which_intensities[i])\n",
    "        \n",
    "    bin_intensities_count = []\n",
    "    \n",
    "    for i in range(len(mz_bins)):\n",
    "        bin_intensities_count.append(len(mz_bins[i]))\n",
    "        \n",
    "    if chart:\n",
    "        fig = go.Figure()\n",
    "\n",
    "        fig.add_trace(go.Bar(y=bin_intensities_count,\n",
    "                             x=list(range(num_of_mz_bins)),\n",
    "                             marker_color='red'))\n",
    "\n",
    "        fig.show()\n",
    "        \n",
    "    return mz_bins, bin_intensities_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# For each discrete m/z, discretize the existing intensities\n",
    "#\n",
    "\n",
    "def discretize_histograms(mz_bins, intensity_cut_off, analyze_tail=False, max_intensity=None, num_of_intensities_bins=NUM_OF_INTENSITIES_BINS_FOR_CHART):\n",
    "    \n",
    "    bin_histograms = []\n",
    "    \n",
    "    if analyze_tail:\n",
    "        range_description=(intensity_cut_off, max_intensity)        \n",
    "    else:\n",
    "        range_description=(0.0, intensity_cut_off)\n",
    "\n",
    "    print(\"analyze_tail={}, defined range={}\".format(analyze_tail, range_description))\n",
    "        \n",
    "        \n",
    "    for i in range(len(mz_bins)):\n",
    "        histogram, bin_edges = np.histogram(mz_bins[i], bins=num_of_intensities_bins, range=range_description)\n",
    "        bin_histograms.append(histogram)\n",
    "        \n",
    "    expanded_data = np.zeros((NUM_OF_MZ_BINS, num_of_intensities_bins))\n",
    "    \n",
    "    for i in range(len(mz_bins)):\n",
    "        expanded_data[i] = bin_histograms[i]\n",
    "        \n",
    "    return expanded_data, histogram, bin_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mz_intensity_count_chart(expanded_data, intensity_bin_edges, intensity_cut_off=None, max_mz=NUM_OF_MZ_BINS, max_count=MAX_COUNT, chart_title=None):\n",
    "    \n",
    "    if intensity_cut_off:\n",
    "        selected_intensity_bin_edges = intensity_bin_edges[intensity_bin_edges < intensity_cut_off]\n",
    "    else:\n",
    "        selected_intensity_bin_edges = intensity_bin_edges\n",
    "        \n",
    "    fig = go.Figure(data=[go.Surface(y=list(range(max_mz)), x=selected_intensity_bin_edges, z=expanded_data)])\n",
    "\n",
    "    fig.update_traces(contours_z=dict(show=True, usecolormap=True,\n",
    "                                  highlightcolor=\"limegreen\", project_z=True))\n",
    "    \n",
    "    max_position = np.unravel_index(np.argmax(expanded_data), expanded_data.shape)\n",
    "    \n",
    "    print(\"Maximum value={}; position x={}, y={}\".format(expanded_data[max_position[0]][max_position[1]], max_position[1], max_position[0]))\n",
    "    \n",
    "    \n",
    "    fig.update_layout(title=chart_title,\n",
    "                      width=1500, \n",
    "                      height=1000,\n",
    "                      scene = dict(\n",
    "                          yaxis = dict(nticks=20, range=[0, max_mz], title=\"m/z\"),\n",
    "                          xaxis = dict(nticks=50, range=[selected_intensity_bin_edges[0], selected_intensity_bin_edges[-1]], title=\"intensities\"), \n",
    "                          zaxis = dict(nticks=50, range=[0, max_count], title=\"count\")\n",
    "                      ),\n",
    "                      annotations = [dict(\n",
    "                          x = max_position[1],\n",
    "                          y = max_position[0],\n",
    "                          showarrow = True,\n",
    "                          text = \"maximum\",\n",
    "                          visible = True,\n",
    "                          startarrowsize = 10\n",
    "                      )]\n",
    "                     )\n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_dataset(dataset_filename, intensity_percentile_cut_off_index=19, focused_percentile_index=18, analyze_tail=False, dataset_type=\"train\"):\n",
    "\n",
    "    with open(dataset_filename, \"rb\") as inputFile:\n",
    "        dataset = pickle.load(inputFile)\n",
    "\n",
    "    dataset[\"normalizationParameters\"]\n",
    "\n",
    "    # Analyze mz x intensity values\n",
    "\n",
    "    all_mz_np, all_intensities_np, mz_results, intensities_results, peaks_count_results = group_and_describe_dataset(dataset)\n",
    "\n",
    "    intensity_cut_off = intensities_results['percentiles'][intensity_percentile_cut_off_index][1]\n",
    "\n",
    "    intensity_cut_off_percentage = intensities_results['percentiles'][intensity_percentile_cut_off_index][0]\n",
    "    \n",
    "    print(\"\\nValidation intensity at {}={}\\n\".format(intensity_cut_off_percentage, intensity_cut_off))\n",
    "\n",
    "    if analyze_tail:\n",
    "        intensities_indexes = all_intensities_np > intensity_cut_off\n",
    "    else:\n",
    "        intensities_indexes = all_intensities_np <= intensity_cut_off\n",
    "\n",
    "    selected_mz = all_mz_np[intensities_indexes]\n",
    "    selected_intensities = all_intensities_np[intensities_indexes]\n",
    "\n",
    "    \n",
    "    # Group all the intensities in a given discretized m/z ― m/z discretization based only on the rounded m/z value (nominal mass)\n",
    "\n",
    "    mz_bins, bin_intensities_count = allocate_intensities_to_mz_bins(selected_mz, selected_intensities)\n",
    "\n",
    "    \n",
    "    # Graph the count of available intensities per discrete m/z\n",
    "\n",
    "    expanded_data, _, intensities_bin_edges = discretize_histograms(mz_bins, intensity_cut_off, analyze_tail, intensities_results['max'])\n",
    "\n",
    "    print(\"intensities_results max={}\".format(intensities_results['max']))\n",
    "    print(\"intensities_bin_edges={}\".format(intensities_bin_edges))\n",
    "    \n",
    "    create_mz_intensity_count_chart(expanded_data, \n",
    "                                    intensities_bin_edges, \n",
    "                                    max_count=np.max(expanded_data), \n",
    "                                    chart_title=\"Count of m/z (all) x intensity ({} {}) ― {} dataset (PXD000561, q < 0.01, pvalue 10%)\".format(\"after\" if analyze_tail else \"up to\",\n",
    "                                                                                                                                               intensity_cut_off_percentage,\n",
    "                                                                                                                                               dataset_type))\n",
    "\n",
    "    focused_percentile = intensities_results['percentiles'][focused_percentile_index][0]\n",
    "\n",
    "    if analyze_tail:\n",
    "        focused_intensity_cut_off = np.percentile(selected_intensities, focused_percentile)\n",
    "        focused_mz_cut_off = int(np.percentile(selected_mz, focused_percentile))\n",
    "        \n",
    "        print(\"focused_percentile={}, focused_intensity_cut_off={}, focused_mz_cut_off={}\".format(focused_percentile, focused_intensity_cut_off, focused_mz_cut_off))\n",
    "        print(\"filtered bins edges={}\".format(intensities_bin_edges[intensities_bin_edges < focused_intensity_cut_off]))\n",
    "    else:\n",
    "        focused_intensity_cut_off = intensities_results['percentiles'][focused_percentile_index][1]\n",
    "        focused_mz_cut_off = int(mz_results['percentiles'][focused_percentile_index][1])\n",
    "\n",
    "        \n",
    "    expanded_data, _, intensities_bin_edges = discretize_histograms(mz_bins, intensity_cut_off, analyze_tail, focused_intensity_cut_off)\n",
    "        \n",
    "        \n",
    "    create_mz_intensity_count_chart(expanded_data, \n",
    "                                    intensities_bin_edges, \n",
    "                                    max_mz=focused_mz_cut_off, \n",
    "                                    max_count=np.max(expanded_data), \n",
    "                                    chart_title=\"Count of m/z (up to {}) x intensity (up to {}) ― {} dataset (PXD000561, q < 0.01, pvalue 10%)\".format(focused_percentile,\n",
    "                                                                                                                                                       focused_percentile,\n",
    "                                                                                                                                                       dataset_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "explore_dataset(TRAINING_DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "explore_dataset(TRAINING_DATASET, analyze_tail=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "explore_dataset(VALIDATION_DATASET, dataset_type=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "explore_dataset(VALIDATION_DATASET, analyze_tail=True, dataset_type=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "explore_dataset(TEST_DATASET, dataset_type=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "explore_dataset(TEST_DATASET, analyze_tail=True, dataset_type=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize some spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TRAINING_DATASET, \"rb\") as inputFile:\n",
    "    dataset = pickle.load(inputFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_mz = []\n",
    "all_intensities = []\n",
    "\n",
    "all_sequences = []\n",
    "\n",
    "peaks_count = []\n",
    "\n",
    "for key, same_sequence in dataset['spectra'].items():\n",
    "    for spectrum in same_sequence:\n",
    "        all_mz.append(spectrum['nzero_peaks'][:, 0] * dataset['normalizationParameters']['mz_std'] + dataset['normalizationParameters']['mz_mean'])\n",
    "        all_intensities.append(spectrum['nzero_peaks'][:, 1] * dataset['normalizationParameters']['intensity_std'] + dataset['normalizationParameters']['intensity_mean'])\n",
    "        peaks_count.append(spectrum['nzero_peaks'].shape[0])\n",
    "        all_sequences.append(key)\n",
    "\n",
    "print (\"m/z length={}\".format(len(all_mz)))\n",
    "print (\"intensities length={}\".format(len(all_intensities)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_spectrum(spectrum_index, add_peak_intensities=False, output_folder=None):\n",
    "    \n",
    "    # normalized_intensities = np.sqrt(all_intensities[0] / max(all_intensities[0]))\n",
    "    normalized_intensities = all_intensities[spectrum_index] / max(all_intensities[spectrum_index])\n",
    "\n",
    "    print(\"Number of peaks of spectrum={}: {}\".format(spectrum_index, len(normalized_intensities)))\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(go.Bar(y=normalized_intensities,\n",
    "                         x=all_mz[spectrum_index],\n",
    "                         width=0.2,\n",
    "                         marker_color='black'))\n",
    "\n",
    "    fig.update_layout(title = \"Spectrum {} ― Peptide: {}\".format(spectrum_index, all_sequences[spectrum_index]),\n",
    "                      width = 10000, \n",
    "                      height = 500,\n",
    "                      xaxis = dict(\n",
    "                          range = [min(all_mz[spectrum_index]) - 10, max(all_mz[spectrum_index]) + 10],\n",
    "                          tickmode = \"linear\",\n",
    "                          tick0 = 0.0,\n",
    "                          dtick = 2.0\n",
    "                      )\n",
    "                     )\n",
    "\n",
    "    if add_peak_intensities:\n",
    "        annotations=[dict(\n",
    "                        x = xi,\n",
    "                        y = yi + ydelta,\n",
    "                        text = str(round(xi.item(), 7)),\n",
    "                        xanchor = \"auto\",\n",
    "                        yanchor = \"auto\",\n",
    "                        font=dict(size=8),\n",
    "                        showarrow = False,\n",
    "                     ) for xi, yi, ydelta in zip(all_mz[spectrum_index], \n",
    "                                                 normalized_intensities, \n",
    "                                                 ([0.02, 0.04] * (round(len(normalized_intensities) / 2) + 1))[:len(normalized_intensities)])\n",
    "                    ]\n",
    "\n",
    "        fig.update_layout(annotations = annotations)\n",
    "        \n",
    "    fig.show()\n",
    "    \n",
    "    if output_folder:\n",
    "        fig.write_html(os.path.join(output_folder, \"spectra_{}_{}.html\".format(spectrum_index, all_sequences[spectrum_index])))\n",
    "        fig.write_image(os.path.join(output_folder, \"spectra_{}_{}.png\".format(spectrum_index, all_sequences[spectrum_index])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectra_to_plot = random.sample(range(len(all_mz)), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in spectra_to_plot:\n",
    "    plot_spectrum(i, output_folder=OUTPUT_FOLDER) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore intermediate .pkl file information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BACKUP=\"/media/eduseiti/bigdata02/unicamp/doutorado/bootstrap.pytorch/data/mixedSpectraCrux/sequences/Adult_Frontalcortex_bRP_Elite_85_crux_q0.01_pvalue-63.91983.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(BACKUP, \"rb\") as inputFile:\n",
    "    testBackup = pickle.load(inputFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testBackup.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testBackup['normalizationParameters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze linfeng intensities, to look for winsorizing effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LINFENG_62=\"/media/eduseiti/bigdata02/unicamp/doutorado/bootstrap.pytorch/data/linfeng_all_q0.01_cell_state/sequences/sample_experiment_v6.2.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(LINFENG_62, \"rb\") as inputFile:\n",
    "    linfeng_62 = pickle.load(inputFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linfeng_62.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linfeng_62['normalizationParameters']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze mz values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_mz = []\n",
    "\n",
    "for key, same_sequence in linfeng_62['spectra'].items():\n",
    "    for spectrum in same_sequence:\n",
    "        all_mz.append(spectrum['nzero_peaks'][:, 0] * linfeng_62['normalizationParameters']['mz_std'] + linfeng_62['normalizationParameters']['mz_mean'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (len(all_mz))\n",
    "\n",
    "all_mz_concatenated = torch.cat(all_mz)\n",
    "\n",
    "all_mz_np = all_mz_concatenated.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mz_results = describe_data(all_mz_np, histogram_bins = 2000, output_filename = os.path.join(OUTPUT_FOLDER, TEST_LINFENG_DATASET_MZ_INFO))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_mz_concatenated.int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = torch.tensor([1.0, 2.0, 3.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_mz_concatenated[indexes.long()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the validation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EPOCH_DATA_FILENAME=\"/media/eduseiti/data_storage_1TB/unicamp/clustering_linfeng_sample_pvalues/pepmass/last_epoch_data.pkl\"\n",
    "# EPOCH_DATA_RANKS_FILENAME=\"/media/eduseiti/data_storage_1TB/unicamp/clustering_linfeng_sample_pvalues/pepmass/last_epoch_ranks_data.pkl\"\n",
    "# EPOCH_DATA_EMBEDDINGS_FILENAME=\"/media/eduseiti/data_storage_1TB/unicamp/clustering_linfeng_sample_pvalues/pepmass/last_epoch_embeddings_data.pkl\"\n",
    "\n",
    "EPOCH_DATA_FILENAME=\"/media/eduseiti/data_storage_1TB/unicamp/clustering_linfeng_sample_pvalues/identifications_fix/last_epoch_data.pkl\"\n",
    "EPOCH_DATA_RANKS_FILENAME=\"/media/eduseiti/data_storage_1TB/unicamp/clustering_linfeng_sample_pvalues/identifications_fix/last_epoch_ranks_data.pkl\"\n",
    "EPOCH_DATA_EMBEDDINGS_FILENAME=\"/media/eduseiti/data_storage_1TB/unicamp/clustering_linfeng_sample_pvalues/identifications_fix/last_epoch_embeddings_data.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(EPOCH_DATA_FILENAME, \"rb\") as inputFile:\n",
    "    epoch_data = pickle.load(inputFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(EPOCH_DATA_RANKS_FILENAME, \"rb\") as inputFile:\n",
    "    epoch_ranks_data = pickle.load(inputFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(EPOCH_DATA_EMBEDDINGS_FILENAME, \"rb\") as inputFile:\n",
    "    epoch_embeddings_data = pickle.load(inputFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "samples = []\n",
    "\n",
    "rank_index = 0\n",
    "\n",
    "embeddings_starting_index = 0\n",
    "\n",
    "for epoch in epoch_data[-43:]:\n",
    "    \n",
    "#     print(\"Len(epoch['sequence'])={}\".format(len(epoch['sequence'])))\n",
    "    \n",
    "    for i in range(len(epoch['sequence']) // 2):\n",
    "        sample = {}\n",
    "        sample['sequence'] = epoch['sequence'][i * 2]\n",
    "        sample['anchor'] = epoch['index'][i * 2]\n",
    "        sample['positive'] = epoch['index'][i * 2 + 1]\n",
    "        sample['rank'] = epoch_ranks_data[rank_index]\n",
    "        sample['anchor_embeddings'] = epoch_embeddings_data[embeddings_starting_index + i * 2]\n",
    "        sample['postive_embeddings'] = epoch_embeddings_data[embeddings_starting_index + i * 2 + 1]\n",
    "        \n",
    "        samples.append(sample)\n",
    "        \n",
    "        rank_index += 1\n",
    "    \n",
    "    embeddings_starting_index += len(epoch['sequence'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for sample in samples:\n",
    "    if int(sample['rank']) > 7000:\n",
    "        print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST_DATASET_FILE_NEW=\"/media/eduseiti/bigdata02/unicamp/doutorado/bootstrap.pytorch/data/mixedSpectraCrux/sequences/test_mixedSpectraCrux_v5.1.pkl\"\n",
    "\n",
    "TEST_DATASET_FILE_NEW=\"/media/eduseiti/bigdata02/unicamp/doutorado/bootstrap.pytorch/data/mixedSpectraCrux/sequences/test_mixedSpectraCrux_v6.0.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TEST_DATASET_FILE_NEW, \"rb\") as inputLog:\n",
    "    testNew = pickle.load(inputLog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testNew.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testNew['spectraCount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testNew['spectra']['HIHPELR'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testNew['spectra']['ILGIPVIVTEQYPK'][6]['nzero_peaks'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testNew['spectra']['ILGIPVIVTEQYPK'][1]['nzero_peaks'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testNew['spectra']['ILGIPVIVTEQYPK'][6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testNew['spectra']['ILGIPVIVTEQYPK'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "13.68177641947343 * 57.113074883189604 + 4.060123643775214"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "13.681745427979195 * 57.113074883189604 + 4.060123643775214"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3.3978 * 3893.8025116844333 - 2321.0634765625"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testNew['spectra']['ANDTTFGLAAGVFTR'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(testNew['spectra']['ANDTTFGLAAGVFTR'][2]['nzero_peaks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testNew['spectra']['ANDTTFGLAAGVFTR'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(testNew['spectra']['ANDTTFGLAAGVFTR'][0]['nzero_peaks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "13.426558258393413 * 57.113074883189604 + 4.060123643775214"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "13.426592455904299 * 57.113074883189604 + 4.060123643775214"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testNew['spectra']['VLTSLGDAIK'][3]['nzero_peaks'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testNew['spectra']['VLTSLGDAIK'][11]['nzero_peaks'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(testNew['spectra']['VLTSLGDAIK'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(14.725179189788623 * 59.29983008885746) + 4.244749552710952"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(8.508590219375996 * 59.29983008885746) + 4.244749552710952"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(testNew['spectra'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATASET_NEW=\"/media/eduseiti/bigdata02/unicamp/doutorado/bootstrap.pytorch/data/mixedSpectraCrux/sequences/train_mixedSpectraCrux_v5.1.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TRAIN_DATASET_NEW, \"rb\") as inputLog:\n",
    "    trainNew = pickle.load(inputLog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainNew['normalizationParameters']['intensity_percentiles'][-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainNew['spectra']['SGPFGQIFRPDNFVFGQSGAGNNWAK'][0]['nzero_peaks'][:,1][trainNew['spectra']['SGPFGQIFRPDNFVFGQSGAGNNWAK'][0]['nzero_peaks'][:,1] < -0.039] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainNew['spectra']['SGPFGQIFRPDNFVFGQSGAGNNWAK'][0]['nzero_peaks'][:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trainNew['spectra'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainNew['spectra']['SGPFGQIFRPDNFVFGQSGAGNNWAK'][0]['nzero_peaks'][:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainNew['spectra']['SGPFGQIFRPDNFVFGQSGAGNNWAK'][0]['nzero_peaks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_intensities = trainNew['spectra']['SGPFGQIFRPDNFVFGQSGAGNNWAK'][0]['nzero_peaks'][:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_intensities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(torch.cat((all_intensities, trainNew['spectra']['SGPFGQIFRPDNFVFGQSGAGNNWAK'][1]['nzero_peaks'][:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(trainNew['spectra']['SGPFGQIFRPDNFVFGQSGAGNNWAK'][0]['nzero_peaks'][:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore training data intensities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_intensities = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sequence in trainNew['spectra']:\n",
    "    for spectrum in trainNew['spectra'][sequence]:\n",
    "        if type(all_intensities) == torch.Tensor:\n",
    "            all_intensities = torch.cat((all_intensities, spectrum['nzero_peaks'][:, 1]))\n",
    "        else:\n",
    "            all_intensities = spectrum['nzero_peaks'][:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intensities_histogram, intensities_bin_edges = np.histogram(all_intensities, bins=100000)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Bar(y=intensities_histogram[:100],\n",
    "                     x=intensities_bin_edges[:100],\n",
    "                     marker_color='red'))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intensities_bin_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainNew['normalizationParameters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_intensities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_intensities.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_intensities.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.describe(all_intensities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = np.percentile(all_intensities, np.round(np.arange(0.0, 100.0, 0.05), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(2000):\n",
    "    print(\"Percentile={}, intensity={}\".format(np.round(i * 0.05, 2), result[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Bar(y=result,\n",
    "                     x=np.round(np.arange(0.0, 100.0, 0.05), 2),\n",
    "                     marker_color='red'))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2 = np.percentile(all_intensities, np.round(np.arange(0.0, 100.0, 1.0), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    print(\"Percentile={}, intensity={}\".format(np.round(i, 2), result2[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result3 = np.percentile(all_intensities, np.round(np.arange(0.0, 100.0, 0.01), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(result3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10000):\n",
    "    print(\"Percentile={}, intensity={}\".format(np.round(i * 0.01, 2), result3[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.zeros([2, 3, 2, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.tensor([[[[0, 0, 0, 0], [1, 1, 1, 1]], [[2, 2, 2, 2], [3, 3, 3, 3]], [[4, 4, 4, 4], [5, 5, 5, 5]]], [[[6, 6, 6, 6], [7, 7, 7, 7]], [[8, 8, 8, 8], [9, 9, 9, 9]], [[10, 10, 10, 10], [11, 11, 11, 11]]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tam = torch.tensor([1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1[range(t1.shape[0]), 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tam - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
